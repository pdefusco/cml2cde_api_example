{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bad76097-31ad-4573-b57c-ae4ed635e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import cml2cde.cdeconnection as cde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b716bc4c-4e51-41c1-8d8a-778b67111241",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Required Env Variables ###\n",
    "# 1. JOBS API URL for CDE Virtual Cluster you ship the PySpark Job to (you can copy this from the CDE Cluster Service page \n",
    "# 2. CDE Resource Name (This is something you can make up at will)\n",
    "# 3. CDE Job Script (This is a PySpark script you can store anywhere in your local CML Project - an example is provided as /home/cdsw/example_spark_jobs/jobs/sql.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90061d2b-a02d-4f4c-86f3-06fd8faeeccc",
   "metadata": {},
   "source": [
    "#### Connecting to the CDE Virtual Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a40dd490-6a4f-45a4-8740-d08d07de22a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "JOBS_API_URL = \"https://z4xgdztf.cde-6fr6l74r.go01-dem.ylcu-atmi.cloudera.site/dex/api/v1\"\n",
    "WORKLOAD_USER = os.environ[\"WORKLOAD_USER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af91a2b-2b19-444f-b6fb-9da9fe40bcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cde_connection = cde.CdeConnection(JOBS_API_URL, WORKLOAD_USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed350541-81f3-46e5-99cb-04630838f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Workload Password should be set automatically for each user. \n",
    "#If you encounter an error, set the variable directly in the notebook or as a CML Project Environment Variable\n",
    "TOKEN = cde_connection.set_cde_token(os.environ[\"WORKLOAD_PASSWORD\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c3e925-ae4c-4945-8f23-b86807b701fd",
   "metadata": {},
   "source": [
    "#### Creating a CDE Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62862d0b-58c6-472f-acce-269c0230e76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CDE_RESOURCE_NAME = \"example-spark-jobs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8224cff1-6a76-423e-ba70-fe96a5e1ebdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Creating Resource example-spark-jobs\n",
      "409\n",
      "{\"status\":\"error\",\"message\":\"resource with name already exists\"}\n"
     ]
    }
   ],
   "source": [
    "cde_connection.create_cde_resource(TOKEN, CDE_RESOURCE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f5aa84f-f673-43d5-a04f-3567cbe3f126",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYSPARK_EXAMPLE_LOCAL_PATH = \"/home/cdsw/example_spark_jobs/jobs/\"\n",
    "PYSPARK_EXAMPE_SCRIPT_NAME = \"sql.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743166ad-0ca5-4f7b-91c3-435df2ed4ead",
   "metadata": {},
   "source": [
    "#### Uploading the Example PySpark script to the CDE Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03f31af5-579c-4f42-afa2-e9190b892b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading File sql.py to CDE Resource example-spark-jobs\n",
      "Response Status Code 201\n",
      "Uploading File sql.py to CDE Resource example-spark-jobs has Succeeded\n"
     ]
    }
   ],
   "source": [
    "# Caution: this method directly overwrites any existing files with the same PYSPARK_EXAMPE_SCRIPT_NAME within the CDE Resource if present\n",
    "\n",
    "cde_connection.upload_file(CDE_RESOURCE_NAME, PYSPARK_EXAMPLE_LOCAL_PATH, PYSPARK_EXAMPE_SCRIPT_NAME, TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47063364-08ef-4205-9f2f-547d36095612",
   "metadata": {},
   "source": [
    "#### Creating (Declaring but not Executing) the CDE Job from the CDE Resource Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c092c91-4d01-484c-95ac-f40aa61055c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The CDE Job Name as it will appear in the CDE Jobs UI - Not to be confused with the Script Name above \"PYSPARK_EXAMPE_SCRIPT_NAME\"\n",
    "CDE_JOB_NAME = \"simple_spark_sql\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f2e48c7-b452-40f9-aa51-0518a8672ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Creating CDE Spark Job simple_spark_sql with Script sql.py\n",
      "500\n",
      "{\"status\":\"error\",\"message\":\"job with name already exists\"}\n"
     ]
    }
   ],
   "source": [
    "cde_connection.create_spark_job_from_resource(TOKEN, CDE_JOB_NAME, CDE_RESOURCE_NAME, PYSPARK_EXAMPE_SCRIPT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caf9494-3790-42bc-854b-b97d454b8808",
   "metadata": {},
   "source": [
    "##### Spark Configurations can be set at CDE Spark Job declaration and are not mandatory.\n",
    "##### The create_spark_job_from_resource() method used above allows you to pass an optional argument \"spark_confs\"\n",
    "##### e.g. \n",
    "        spark_confs_example = { \n",
    "                  \"spark.dynamicAllocation.maxExecutors\": \"6\",\n",
    "                  \"spark.dynamicAllocation.minExecutors\": \"2\",\n",
    "                  \"spark.executor.extraJavaOptions\": \"-Dsun.security.krb5.debug=true -Dsun.security.spnego.debug=true\",\n",
    "                  \"spark.hadoop.fs.s3a.metadatastore.impl\": \"org.apache.hadoop.fs.s3a.s3guard.NullMetadataStore\",\n",
    "                  \"spark.kubernetes.memoryOverheadFactor\": \"0.2\",\n",
    "                  \"spark.pyspark.python\": \"python3\"\n",
    "                  \"spark.rpc.askTimeout\": \"600\",\n",
    "                  \"spark.sql.shuffle.partitions\": \"48\",\n",
    "                  \"spark.yarn.access.hadoopFileSystems\": \"s3a://your_data_lake_here\"\n",
    "                }                 \n",
    "cde_connection.create_spark_job_from_resource(TOKEN, CDE_JOB_NAME, CDE_RESOURCE_NAME, PYSPARK_EXAMPE_SCRIPT_NAME, **spark_confs=spark_confs_example**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c713a0-678c-4962-bdbc-146e28652f76",
   "metadata": {},
   "source": [
    "#### Running the CDE Job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55f874a9-7d3d-448f-9245-e8363de43201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started to Submit Spark Job simple_spark_sql\n",
      "Submitting CDE Spark Job simple_spark_sql has Succeeded\n",
      "This doesn't necessarily mean that the CDE Spark Job has Succeeded\n",
      "Please visit the CDE Job Runs UI to check on CDE Job Status\n"
     ]
    }
   ],
   "source": [
    "#The Following Spark Resource Arguments default to the following values\n",
    "#\"driverCores\": 2, \"driverMemory\": \"4g\", \"executorCores\": 4, \"executorMemory\": \"4g\", \"numExecutors\": 4\n",
    "\n",
    "#You can customize resources with e.g. run_spark_job(TOKEN, CDE_JOB_NAME, driver_cores = 4, driver_memory = \"8g\", executor_cores = 4, executor_memory = \"12g\", num_executors = 10)  \n",
    "\n",
    "cde_connection.run_spark_job(TOKEN, CDE_JOB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ad5eae-1ef5-4e55-a076-85c6d7b425dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
